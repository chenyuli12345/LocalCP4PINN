from utils_uqmd.interface_model import BasePINNModel
from utils_uqmd.utils_layer_BayesianLinearLayer import BayesianLinearLayer as BayesianLinear

import torch
import torch.nn as nn
import math
from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau
##########################################################
# TODO: å°†PINNçš„ç½‘ç»œç»“æ„è½¬æ¢ä¸ºè´å¶æ–¯ç½‘ç»œç»“æ„
##########################################################


class BayesianFeedForwardNN(BasePINNModel):
    """å…·æœ‰è´å¶æ–¯çº¿æ€§å±‚çš„å‰é¦ˆç¥ç»ç½‘ç»œ (for VI)."""
    def __init__(self, input_dim, hidden_dims, output_dim, mu_std, rho, prior_std=1.0, act_func=nn.Tanh()): #å‚æ•°åˆ†åˆ«ä¸ºè¾“å…¥ç»´åº¦ã€éšè—å±‚ç»´åº¦åˆ—è¡¨ã€è¾“å‡ºç»´åº¦ã€muçš„æ ‡å‡†å·®ã€rhoå‚æ•°ã€å…ˆéªŒæ ‡å‡†å·®ï¼ˆè¿™ä¸‰ä¸ªå‚æ•°ç”¨äºè´å¶æ–¯å±‚ï¼‰ã€æ¿€æ´»å‡½æ•°
        super().__init__()
        if isinstance(hidden_dims, int):
            hidden_dims = [hidden_dims]
        layers = []
        prev_dim = input_dim
        # éå† hidden_dimsï¼ˆéšè—å±‚ç»´åº¦åˆ—è¡¨ï¼‰ï¼Œåœ¨æ¯ä¸€å±‚éƒ½æ”¾ç½®ä¸€ä¸ª BayesianLinear
        for h in hidden_dims:
            layers.append(BayesianLinear(prev_dim, h, mu_std, rho, prior_std))  # in_feat, out_feat, prior_std
            layers.append(act_func)
            prev_dim = h
        # è¾“å‡ºå±‚ï¼ˆä¹Ÿæ˜¯è´å¶æ–¯å±‚ï¼‰
        layers.append(BayesianLinear(prev_dim, output_dim, mu_std, rho, prior_std))
        self.layers = nn.ModuleList(layers)  # not using Sequential because it's a mix of custom and activations

    def forward(self, x, sample: bool = True):
        out = x
        for layer in self.layers:
            # [è´å¶æ–¯å±‚, æ¿€æ´»å‡½æ•°, è´å¶æ–¯å±‚, æ¿€æ´»å‡½æ•°, ..., æ¿€æ´»å‡½æ•°, è´å¶æ–¯å±‚]
            out = layer(out)  # æ¯ä¸€ä¸ªéƒ½æ˜¯è´å¶æ–¯å±‚æˆ–è€…æ¿€æ´»å‡½æ•°
        return out

    def kl_divergence(self):
        #åŠ å’Œæ¯ä¸€ä¸ªè´å¶æ–¯çº¿å½¢å±‚çš„KLæ•£åº¦
        kl_total = 0.0
        for layer in self.layers:
            if isinstance(layer, BayesianLinear):
                kl_total += layer.kl_divergence()
        return kl_total

#è¿™æ®µä»£ç nll_gaussianè®¡ç®—çš„æ˜¯é«˜æ–¯è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNegative Log-Likelihood, NLLï¼‰ã€‚åœ¨è´å¶æ–¯æ·±åº¦å­¦ä¹ ä¸­ä¸ä»…ä»…æ˜¯è®©é¢„æµ‹å€¼é€¼è¿‘çœŸå®å€¼ï¼ˆåƒMSEé‚£æ ·ï¼‰ï¼Œè€Œæ˜¯ä»æ¦‚ç‡çš„è§’åº¦çœ‹å¾…é—®é¢˜ï¼šå‡è®¾çœŸå®æ•°æ®æ˜¯ç”±æ¨¡å‹é¢„æµ‹çš„å‡å€¼åŠ ä¸ŠæŸç§é«˜æ–¯å™ªå£°äº§ç”Ÿçš„ï¼ˆä¹Ÿå°±æ˜¯ç›¸å½“äºå‡è®¾è§‚æµ‹æ•°æ®y_trueæœä»ä»¥æ¨¡å‹é¢„æµ‹y_predä¸ºå‡å€¼ã€ä»¥data_noise_guessä¸ºæ ‡å‡†å·®çš„é«˜æ–¯åˆ†å¸ƒï¼‰ï¼Œé‚£ä¹ˆåœ¨è¿™ä¸ªå‡è®¾ä¸‹ï¼Œè§‚æµ‹åˆ°å½“å‰æ•°æ®çš„æ¦‚ç‡æœ‰å¤šå¤§ï¼Ÿè¦æœ€å¤§åŒ–è¿™ä¸ªæ¦‚ç‡ï¼ˆä¼¼ç„¶ï¼‰ï¼Œç­‰ä»·äºæœ€å°åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNLLï¼‰ã€‚
#å‡è®¾æˆ‘ä»¬çš„è§‚æµ‹æ•°æ®ğ‘¦_ğ‘¡ğ‘Ÿğ‘¢ğ‘’æœä»ä»¥æ¨¡å‹é¢„æµ‹å€¼ğ‘¦_ğ‘ğ‘Ÿğ‘’ğ‘‘ä¸ºå‡å€¼ï¼Œä»¥data_noise_guess(ğœ)ä¸ºæ ‡å‡†å·®çš„æ­£æ€åˆ†å¸ƒï¼šğ‘(ğ‘¦_ğ‘¡ğ‘Ÿğ‘¢ğ‘’âˆ£ğ‘¦_ğ‘ğ‘Ÿğ‘’ğ‘‘,ğœ)=1/(2ğœ‹ğœ)^(1/2)*expâ¡(âˆ’(ğ‘¦_ğ‘¡ğ‘Ÿğ‘¢ğ‘’-ğ‘¦_ğ‘ğ‘Ÿğ‘’ğ‘‘)^2/(2*ğœ^2)), ä¸ºäº†è®¡ç®—æ–¹ä¾¿ï¼Œæˆ‘ä»¬å¯¹è¿™ä¸ªæ¦‚ç‡å¯†åº¦å‡½æ•°å–å¯¹æ•°ï¼šlogâ¡(1/(2ğœ‹ğœ^2)^(1/2))âˆ’(ğ‘¦_ğ‘¡ğ‘Ÿğ‘¢ğ‘’-ğ‘¦_ğ‘ğ‘Ÿğ‘’ğ‘‘)^2/(2*ğœ^2),å…¶ä¸­ç¬¬ä¸€é¡¹ä¸ºå¸¸æ•°é¡¹ã€‚å› ä¸ºåœ¨è®­ç»ƒæ¨¡å‹æ—¶æ˜¯è¦â€œæœ€å°åŒ–æŸå¤±â€ï¼Œè€Œä¸æ˜¯â€œæœ€å¤§åŒ–å¯¹æ•°æ¦‚ç‡â€ï¼Œæ‰€ä»¥å–è´Ÿå·ï¼Œå˜æˆäº† Negative Log-Likelihood (NLL)ï¼š(ğ‘¦_ğ‘¡ğ‘Ÿğ‘¢ğ‘’-ğ‘¦_ğ‘ğ‘Ÿğ‘’ğ‘‘)^2/(2*ğœ^2)-1/2*logâ¡(1/(2ğœ‹ğœ^2))ï¼Œå…¶ä¸­ç¬¬ä¸€é¡¹ä¸ºæ•°æ®æ‹Ÿåˆé¡¹ï¼Œç¬¬äºŒé¡¹ä¸ºå¸¸æ•°é¡¹ï¼ˆä»£ç ä¸­è¢«æ³¨é‡Šæ‰ï¼‰
#data_noise_guessçš„ç‰©ç†æ„ä¹‰ï¼Œè¿™ä¸ªå‚æ•°éå¸¸å…³é”®ï¼Œå®ƒä»£è¡¨äº†æˆ‘ä»¬å¯¹æ•°æ®æœ¬èº«è´¨é‡çš„é¢„è®¾ï¼ˆå³å¶ç„¶ä¸ç¡®å®šæ€§/Aleatoric Uncertaintyï¼‰ï¼šLossâˆMSE/ğœ^2ã€‚å¦‚æœdata_noise_guess(Ïƒ)è®¾å¾—å¾ˆå°ï¼Œè¿™æ„å‘³ç€ä½ éå¸¸ä¿¡ä»»æ•°æ®ï¼Œè®¤ä¸ºæ•°æ®éå¸¸ç²¾ç¡®ï¼Œæ²¡ä»€ä¹ˆå™ªå£°ï¼Œç»“æœæ˜¯Lossä¼šå˜å¾—éå¸¸å¤§ï¼Œæ¨¡å‹ä¼šå—åˆ°å¼ºçƒˆçš„æƒ©ç½šï¼Œæ‹¼å‘½å»æ‹Ÿåˆæ¯ä¸€ä¸ªæ•°æ®ç‚¹ï¼ˆå®¹æ˜“è¿‡æ‹Ÿåˆï¼‰ã€‚åä¹‹åˆ™è®¤ä¸ºæ•°æ®å¾ˆâ€œè„â€ï¼Œå«æœ‰å¾ˆå¤šå™ªå£°ï¼Œç»“æœæ˜¯MSEè¢«é™¤ä»¥äº†ä¸€ä¸ªå¤§æ•°ï¼ŒLosså˜å°äº†ã€‚æ¨¡å‹ä¼šè§‰å¾—â€œåæ­£æ•°æ®ä¹Ÿä¸å‡†ï¼Œå·®ä¸å¤šå¯¹é½å°±è¡Œäº†â€ï¼Œæ­¤æ—¶æ¨¡å‹æ›´å€¾å‘äºå¬ä»KLæ•£åº¦ï¼ˆå…ˆéªŒï¼‰çš„æŒ‡æŒ¥ï¼Œä¿æŒç®€å•çš„å¹³æ»‘æ›²çº¿ï¼ˆå®¹æ˜“æ¬ æ‹Ÿåˆï¼‰ã€‚
#æ™®é€šçš„MSE Lossåªæ˜¯å•çº¯åœ°è¡¡é‡è·ç¦»ã€‚NLL (Gaussian)æ˜¯åœ¨è¡¡é‡æ¦‚ç‡ï¼Œå®ƒæŠŠè¯¯å·®é¡¹å’Œæ•°æ®çš„å™ªå£°æ–¹å·®è”ç³»åœ¨äº†ä¸€èµ·ã€‚
    def nll_gaussian(self, y_pred, y_true, data_noise_guess=1.0): #å‚æ•°é¡¹ä¸ºé¢„æµ‹å€¼ã€çœŸå®å€¼ã€æ•°æ®å™ªå£°ï¼ˆé»˜è®¤1ï¼Œè®¾ç½®è¶Šå¤§è¶Šå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œè®¾ç½®è¶Šå°è¶Šå®¹æ˜“æ¬ æ‹Ÿåˆï¼‰
        # omit constant
        mse = (y_pred - y_true).pow(2).sum()
        # const = N * torch.log(torch.tensor(2 * math.pi * data_noise_guess ** 2)) #å¸¸æ•°é¡¹æ³¨é‡Šæ‰äº†
        nll = 0.5 * (mse / (data_noise_guess ** 2))
        return nll

#æœ€åçš„è´å¶æ–¯ç¥ç»ç½‘ç»œçš„æ€»æŸå¤±æ˜¯NLLå’ŒKLæ•£åº¦çš„å’Œ

