from utils_uqmd.interface_layer import BaseLayer
import torch
import torch.nn as nn
import math

#è´å¶æ–¯çº¿æ€§å±‚çš„å®žçŽ°ï¼Œè´å¶æ–¯çš„æƒé‡å’Œåç½®æœä»Žæ¦‚çŽ‡åˆ†å¸ƒï¼ˆé€šå¸¸æ˜¯é«˜æ–¯åˆ†å¸ƒï¼‰ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ç›´æŽ¥å­¦ä¹ æƒé‡å€¼ï¼Œè€Œæ˜¯å­¦ä¹ æƒé‡åˆ†å¸ƒçš„å‚æ•°ï¼ˆå‡å€¼Î¼å’Œæ ‡å‡†å·®Ïƒï¼‰

# def default_mu_rho(in_features, out_features,
#                    mu_std=0.1, rho=-3.0,
#                    mu_mean=0.0,
#                    prior_std=1.0):

#     # Weights and Biases Distribution Initialization
#     weight_mu = nn.Parameter(torch.empty(out_features, in_features).normal_(mu_mean, mu_std))
#     weight_rho = nn.Parameter(torch.empty(out_features, in_features).fill_(rho))
#     bias_mu = nn.Parameter(torch.empty(out_features).normal_(mu_mean, mu_std))
#     bias_rho = nn.Parameter(torch.empty(out_features).fill_(rho))

#     # Std of the prior distribution
#     prior_std = prior_std

#     return weight_mu, weight_rho, bias_mu, bias_rho, prior_std


import torch
import torch.nn as nn

#åˆå§‹åŒ–å‡½æ•°ï¼Œç”¨äºŽåˆå§‹åŒ–è´å¶æ–¯å±‚ä¸­éœ€è¦å­¦ä¹ çš„å‚æ•°ï¼ŒåŒ…æ‹¬æƒé‡å’Œåç½®çš„å‡å€¼ï¼ˆmuï¼‰å’Œrhoï¼ˆç”¨äºŽè®¡ç®—æ ‡å‡†å·®ï¼‰
def default_mu_rho(in_features, out_features,
                  mu_std=0.1,        # å…¼å®¹æ—§APIï¼Œå®žé™…ä»£ç ä¸­æœªè¢«ä½¿ç”¨
                  rho=-3.0,         #å†³å®šåˆå§‹æ ‡å‡†å·®çš„å‚æ•°
                  prior_std=1.0,   #å…ˆéªŒåˆ†å¸ƒçš„æ ‡å‡†å·®
                  gain=None):
    if gain is None:
        #å¦‚æžœåŽç»­ä½¿ç”¨ tanh æ¿€æ´»å‡½æ•°ï¼ŒæŽ¨èä½¿ç”¨ 5/3 çš„å¢žç›Šï¼Œä¿æŒæ–¹å·®ç¨³å®š
        gain = nn.init.calculate_gain('tanh')  # 5/3

    #æƒé‡çš„å‡å€¼
    weight_mu = nn.Parameter(torch.empty(out_features, in_features))
    nn.init.xavier_uniform_(weight_mu, gain=gain)

    #åç½®çš„å‡å€¼
    bias_mu = nn.Parameter(torch.zeros(out_features))

    #Rhoå‚æ•°ï¼Œç”¨äºŽè®¡ç®—æƒé‡å’Œåç½®çš„æ ‡å‡†å·®ï¼ŒåŒæ—¶é€šè¿‡softpluså‡½æ•°ç¡®ä¿æ ‡å‡†å·®ä¸ºæ­£å€¼
    weight_rho = nn.Parameter(torch.full((out_features, in_features), rho))
    bias_rho   = nn.Parameter(torch.full((out_features,), rho))

    return weight_mu, weight_rho, bias_mu, bias_rho, float(prior_std)

#è´å¶æ–¯å±‚çš„æƒé‡wå’Œåæ‰§ä¸æ˜¯ä¸€ä¸ªå›ºå®šçš„æ•°ï¼Œè€Œæ˜¯ä¸€ä¸ªæ¦‚çŽ‡åˆ†å¸ƒï¼ˆé€šå¸¸å‡è®¾ä¸ºé«˜æ–¯åˆ†å¸ƒwâˆ¼N(Î¼,Ïƒ^2)ï¼Œå…¶ä¸­Î¼æ˜¯å‡å€¼ï¼ŒÏƒæ˜¯æ ‡å‡†å·®ï¼‰
class BayesianLinearLayer(BaseLayer): #ç»§æ‰¿è‡ªBaseLayer
    """
    å…¨å› å­åŒ–çš„è´å¶æ–¯çº¿æ€§å±‚ï¼Œåœ¨è°ƒç”¨æ—¶å°†sample=Falseï¼Œå³å¯å¾—åˆ°åŽéªŒåˆ†å¸ƒçš„å‡å€¼ï¼ˆç¡®å®šæ€§è¾“å‡ºï¼‰ã€‚
    """

    def __init__(self, in_features, out_features,
                 mu_std=0.1, rho=-3.0, prior_std=1.0,
                 initialization=default_mu_rho): #å‚æ•°åˆ†åˆ«ä¸ºè¾“å…¥ç»´åº¦ã€è¾“å‡ºç»´åº¦ã€muçš„æ ‡å‡†å·®(å¥½åƒæ²¡ç”¨ï¼Œåˆå§‹åŒ–æƒé‡å’Œåç½®çš„å‡å€¼Î¼æ—¶ç›´æŽ¥ç”¨0)ã€rhoå‚æ•°ï¼ˆåˆå§‹æ—¶åˆ»çš„rhoå€¼ï¼Œç„¶åŽé€šè¿‡softpluså‡½æ•°å³å˜ä¸ºæœ€ç»ˆæƒé‡å’Œåç½®çš„æ ‡å‡†å·®ï¼‰ã€å…ˆéªŒæ ‡å‡†å·®ï¼ˆåˆå§‹çŠ¶æ€ä¸‹æ¨¡åž‹æœ‰å¤šä¸ç¡®å®šï¼Œå³é¢„è®¾çš„æƒé‡å’Œåç½®çš„é«˜æ–¯åˆ†å¸ƒçš„æ ‡å‡†å·®ï¼Œå‡å€¼ä¸º0ï¼Œé¢„è®¾çš„æ ‡å‡†å·®è¶Šå°ï¼Œç›¸å½“äºŽå¼ºæ­£åˆ™åŒ–ï¼Œå¼ºè¿«æ¨¡åž‹å­¦åˆ°çš„æƒé‡å¿…é¡»éžå¸¸æŽ¥è¿‘0ï¼Œä¸”åˆ†å¸ƒéžå¸¸çª„ï¼›é¢„è®¾çš„æ ‡å‡†å·®è¶Šå¤§ï¼Œç›¸å½“äºŽå¼±æ­£åˆ™åŒ–ï¼Œå…è®¸æ¨¡åž‹å­¦åˆ°çš„æƒé‡è¿™å°±åç¦»0è¾ƒè¿œï¼Œæˆ–è€…æ‹¥æœ‰è¾ƒå¤§çš„ä¸ç¡®å®šæ€§ï¼‰
        super().__init__()
        (self.weight_mu, self.weight_rho,
         self.bias_mu,  self.bias_rho,
         self.prior_std) = initialization(in_features, out_features,
                                          mu_std=mu_std, rho=rho,
                                          prior_std=prior_std) #è°ƒç”¨åˆå§‹åŒ–å‡½æ•°èŽ·å¾—å››ä¸ªå¯å­¦ä¹ çš„å‚æ•°
        self.log2pi = math.log(2 * math.pi) #ä¸€ä¸ªå¸¸æ•°

    # ---------- è¾…åŠ©å‡½æ•° ----------
    @staticmethod
    def _softplus(x):
        #Softpluså‡½æ•°ï¼šå¹³æ»‘çš„ReLUï¼Œå°†(-inf,inf)æ˜ å°„åˆ°(0,inf)
        return torch.log1p(torch.exp(x))

    # ---------- forward ----------
    def forward(self, x, *, sample: bool = True):
        """
        sample=True: è®­ç»ƒæ—¶ä½¿ç”¨ã€‚ä»Žåˆ†å¸ƒä¸­éšæœºé‡‡æ ·æƒé‡ï¼Œæ¨¡æ‹Ÿä¸ç¡®å®šæ€§ã€‚
        sample=False: é¢„æµ‹/æµ‹è¯•æ—¶ä½¿ç”¨ã€‚ç›´æŽ¥ä½¿ç”¨å‡å€¼ï¼Œç›¸å½“äºŽæ™®é€šçš„ç¡®å®šæ€§ç¥žç»ç½‘ç»œã€‚
        """
        if sample: #è®­ç»ƒæ—¶
            w_sigma = self._softplus(self.weight_rho)
            b_sigma = self._softplus(self.bias_rho)
            eps_w   = torch.randn_like(w_sigma)
            eps_b   = torch.randn_like(b_sigma)
            weight  = self.weight_mu + w_sigma * eps_w
            bias    = self.bias_mu  + b_sigma * eps_b
        else:                       # posterior mean
            weight, bias = self.weight_mu, self.bias_mu
        return x.matmul(weight.t()) + bias #æ‰§è¡Œçº¿æ€§å˜æ¢: y = xW^T + b

    # ---------- å½“å‰å±‚KLæ•£åº¦çš„è®¡ç®— ----------
    #å¸Œæœ›åŽéªŒåˆ†å¸ƒï¼ˆå­¦åˆ°çš„ï¼‰ä¸è¦åç¦»å…ˆéªŒåˆ†å¸ƒï¼ˆåˆå§‹å°è±¡ï¼‰å¤ªè¿œï¼Œå› æ­¤åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè®¡ç®—KLæ•£åº¦ä½œä¸ºæ­£åˆ™åŒ–é¡¹ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰ã€‚     
    #ä»£ç ä¸­å‡è®¾æƒé‡çš„â€œå…ˆéªŒåˆ†å¸ƒâ€å’Œâ€œåŽéªŒåˆ†å¸ƒâ€éƒ½æ˜¯é«˜æ–¯åˆ†å¸ƒï¼ˆæ­£æ€åˆ†å¸ƒï¼‰ã€‚å…¶ä¸­åŽéªŒåˆ†å¸ƒq(w)ï¼ˆæ¨¡åž‹å­¦åˆ°çš„ï¼‰ï¼Œå‡å€¼ä¸ºÎ¼ï¼Œæ ‡å‡†å·®ä¸ºÏƒã€‚å…ˆéªŒåˆ†å¸ƒp(w)ï¼ˆæˆ‘ä»¬é¢„è®¾çš„ï¼‰ï¼šå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸ºÏƒ_pï¼ˆä»£ç ä¸­çš„ prior_stdï¼‰ã€‚å¯¹äºŽè¿™ä¸¤ä¸ªå•å˜é‡é«˜æ–¯åˆ†å¸ƒï¼Œå®ƒä»¬ä¹‹é—´çš„ KL æ•£åº¦å…¬å¼æ˜¯ï¼šD_KL(q||p) = ln(Ïƒ_p/Ïƒ) + (Ïƒ^2 + (Î¼ - 0)^2) / (2Ïƒ_p^2) - 1/2ï¼Œè¿™ä¸ªå…¬å¼ç”±ä¸‰éƒ¨åˆ†ç»„æˆã€‚
    #prior_stdï¼šå…ˆéªŒä¿¡å¿µï¼ˆæ­£åˆ™åŒ–å¼ºåº¦ï¼‰ã€‚å«ä¹‰æ˜¯åœ¨è®­ç»ƒå‰äººä¸ºè®¾å®šçš„ã€ç†æƒ³ä¸­çš„æƒé‡åˆ†å¸ƒçš„æ ‡å‡†å·®ã€‚ä½œç”¨æ˜¯å®šä¹‰äº†KLæ•£åº¦ä¸­çš„ç›®æ ‡åˆ†å¸ƒp(w)ï¼šp(w)=N(0,prior_std^2)ã€‚åœ¨è®­ç»ƒä¸­è®¡ç®—KLæ•£åº¦ï¼Œç›®çš„æ˜¯è®©å­¦åˆ°çš„åŽéªŒåˆ†å¸ƒq(w)é è¿‘è¿™ä¸ªå…ˆéªŒåˆ†å¸ƒp(w)ã€‚è¾ƒå°çš„ prior_stdï¼ˆå¦‚0.1ï¼‰ç›¸å½“äºŽå¼ºæ­£åˆ™åŒ–ï¼ˆç±»ä¼¼äºŽå¾ˆå¼ºçš„L2 Weight Decayï¼‰ï¼Œå¼ºè¿«æ¨¡åž‹å­¦åˆ°çš„æƒé‡å¿…é¡»éžå¸¸æŽ¥è¿‘0ï¼Œä¸”åˆ†å¸ƒéžå¸¸çª„ã€‚è¾ƒå¤§çš„prior_stdï¼ˆå¦‚1.0ï¼‰ç›¸å½“äºŽå¼±æ­£åˆ™åŒ–ï¼Œå…è®¸æ¨¡åž‹å­¦åˆ°çš„æƒé‡è¿™å°±åç¦»0è¾ƒè¿œï¼Œæˆ–è€…æ‹¥æœ‰è¾ƒå¤§çš„ä¸ç¡®å®šæ€§.   
    def kl_divergence(self):
        w_sigma = self._softplus(self.weight_rho) #è®¡ç®—åŽéªŒçš„æ ‡å‡†å·®
        b_sigma = self._softplus(self.bias_rho)
        prior_var = self.prior_std ** 2 #å…ˆéªŒåˆ†å¸ƒçš„æ–¹å·®

        #è¿™é‡Œsumçš„ä½œç”¨æ˜¯å› ä¸ºä¸€ä¸ªçº¿æ€§å±‚æœ‰æˆåƒä¸Šä¸‡ä¸ªæƒé‡ï¼ˆWï¼‰å’Œåç½®ï¼ˆbï¼‰ï¼Œæˆ‘ä»¬å‡è®¾æ¯ä¸ªæƒé‡ä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹çš„ï¼Œæ‰€ä»¥æŠŠæ‰€æœ‰æƒé‡çš„ KL æ•£åº¦ç´¯åŠ èµ·æ¥ï¼Œå¾—åˆ°è¿™ä¸€å±‚æ€»çš„â€œä»£ä»·â€
        kl_w = torch.sum(torch.log(self.prior_std / w_sigma) + #ln(Ïƒ_p/Ïƒ)ï¼ˆå¯¹æ¯”ä¸¤ä¸ªåˆ†å¸ƒçš„â€œèƒ–ç˜¦â€ï¼ˆå®½åº¦ï¼‰ã€‚å¦‚æžœæ¨¡åž‹å­¦åˆ°çš„Ïƒéžå¸¸å°ï¼ˆåˆ†å¸ƒéžå¸¸çª„/ç˜¦ï¼‰ï¼Œè¿™ä¸€é¡¹çš„å€¼å°±ä¼šå˜å¾—å¾ˆå¤§ã€‚è¿™ç›¸å½“äºŽåœ¨æƒ©ç½šé‚£äº›è¿‡äºŽâ€œè‡ªä¿¡â€çš„ç¥žç»å…ƒã€‚ï¼‰
                         0.5 * (w_sigma ** 2 + self.weight_mu ** 2) / prior_var #(Ïƒ^2 + Î¼^2) / (2Ïƒ_p^2)ï¼ˆå¯¹æ¯”ä¸¤ä¸ªåˆ†å¸ƒçš„â€œä½ç½®â€ã€‚å¦‚æžœå‡å€¼Î¼ç¦»0å¾ˆè¿œï¼Œæˆ–è€…Ïƒå¾ˆå¤§ï¼Œè¿™ä¸€é¡¹å°±ä¼šå¢žåŠ ã€‚è¿™ç›¸å½“äºŽL2æ­£åˆ™åŒ–ï¼ˆæƒé‡è¡°å‡ï¼‰ï¼Œå®ƒé˜²æ­¢æƒé‡æ•°å€¼ç‚¸è£‚ï¼‰
                         - 0.5 #å¸¸æ•°é¡¹ï¼Œä¿è¯å½“ä¸¤ä¸ªåˆ†å¸ƒå®Œå…¨ä¸€æ ·æ—¶ï¼ŒKLæ•£åº¦ä¸º0
                         )
        kl_b = torch.sum(torch.log(self.prior_std / b_sigma) +
                         0.5 * (b_sigma ** 2 + self.bias_mu ** 2) / prior_var - 0.5)
        return kl_w + kl_b #è¿”å›žè¯¥å±‚çš„æ€»KLæ•£åº¦


# Explain::
# class BayesianLinearLayer(BaseLayer):
#     """ ä¸€å±‚ Bayesian Linear Layer
#         Bayesian Linear layer with Gaussian weight and bias priors and variational posteriors.
#     """

#     def __init__(self, in_features, out_features, mu_std, rho, prior_std, initialization=default_mu_rho):
#         super().__init__()
#         # Mean and log-variance (or rho) for weights and biases as learnable parameters
#         self.in_features = in_features
#         self.out_features = out_features

#         # ------------------------------ Model's Parameters ------------------------------------------
#         # Initialize means (mu) to small random values, and rho to a small negative (so sigma ~ small)
#         # Since Ïƒ must be strictly positive, so we optimize rho, compute Ïƒ by softplus(rho)
#         # So, we are still learning the std Ïƒ, but indirectly
#         (self.weight_mu, self.weight_rho, self.bias_mu, self.bias_rho,
#          self.prior_std) = initialization(in_features, out_features, mu_std=mu_std, rho=rho, prior_std=prior_std)

#         # Prior standard deviation (fixed)
#         self.log2pi = math.log(2 * math.pi)  # for potential use in exact logprob if needed

#     def forward(self, x):
#         # Sample the std  Ïƒ  of the weights and biases (the reparameterization trick)
#         weoight_sigma = torh.log1p(torch.exp(self.weight_rho))  # softplus to ensure positivity
#         bias_sigma = torch.log1p(torch.exp(self.bias_rho))

#         # Sample Îµ âˆ¼ ð’©(0,1) for weights and baises
#         eps_w = torch.randn_like(weight_sigma)
#         eps_b = torch.randn_like(bias_sigma)

#         # Sample from ð’©(mu, sigma^2) through variable transformation
#         # è¿™æ ·, æˆ‘ä»¬å°±èƒ½ update `mu` å’Œ `sigma`(rho)
#         weight = self.weight_mu + weight_sigma * eps_w
#         bias = self.bias_mu + bias_sigma * eps_b

#         # Linear layer computation xWáµ€ + b
#         return x.matmul(weight.t()) + bias  # the utput of this bayesian linear layer

#     def kl_divergence(self):
#         # Compute KL divergence KL[q(w,b) || p(w,b)] for this layer (sum over all weights and biases)
#         # Assuming factorized Gaussian posteriors and Gaussian priors N(0, prior_std^2):contentReference[oaicite:2]{index=2}.
#         weight_sigma = torch.log1p(torch.exp(self.weight_rho))
#         bias_sigma = torch.log1p(torch.exp(self.bias_rho))
#         # KL for each weight: log(prior_sigma/post_sigma) + (post_sigma^2 + mu^2)/(2*prior_sigma^2) - 0.5
#         prior_var = self.prior_std ** 2

#         # Compute the KL value using the formula for Gaussian
#         #   = log(prior_std/posterior_std) + 0.5 * (posterior_std**2 + posterior_mu**2) / [prior_std^2] - 1)
#         # For numerical stability, avoid log of 0 by using weight_sigma (softplus ensures >0)
#         kl_weight = torch.sum(torch.log(self.prior_std / weight_sigma) +
#                               0.5 * (weight_sigma ** 2 + self.weight_mu ** 2) / prior_var - 0.5)
#         kl_bias = torch.sum(torch.log(self.prior_std / bias_sigma) +
#                             0.5 * (bias_sigma ** 2 + self.bias_mu ** 2) / prior_var - 0.5)
#         return kl_weight + kl_bias